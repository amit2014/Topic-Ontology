{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('uncleaned-10k-articles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(text): \n",
    "    lowers = text.lower()\n",
    "    ## Python 2.7 solution is easier:\n",
    "    ## no_punctuation = lowers.translate(string.punctuation)\n",
    "    translator = str.maketrans({key: ' ' for key in string.punctuation}) #\n",
    "    no_punctuation = lowers.translate(translator)# \n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "#this requires nltk's stopword corpus\n",
    "# import nltk \n",
    "# then run nltk.download() \n",
    "# and then download the stopwords corpus\n",
    "def remove_stop_words(tokens): \n",
    "    filtered = [w for w in tokens if (not w in stopwords.words('english')) and (not 'http' in w)]\n",
    "    return filtered\n",
    "\n",
    "def stem(tokens):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(PorterStemmer().stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def text_clean(text):\n",
    "    lowers = text.lower()\n",
    "    translator = str.maketrans({key: ' ' for key in string.punctuation}) #\n",
    "    no_punctuation = lowers.translate(translator)# \n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    filtered_stemmed = [PorterStemmer().stem(w) for w in tokens if (not w in stopwords.words('english')) and (not 'http' in w)]\n",
    "    return filtered_stemmed   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfiltered:\n",
      " [('the', 111), ('firestone', 76), ('of', 53), ('in', 47), ('and', 40), ('to', 38), ('ref', 32), ('a', 29), ('tires', 24), ('company', 23), ('was', 23), ('they', 22), ('ford', 21), ('tire', 19), ('that', 19), ('http', 18), ('for', 15), ('on', 15), ('cite', 15), ('title', 15)]\n",
      "\n",
      "filtered:\n",
      " [('firestone', 76), ('ref', 32), ('tires', 24), ('company', 23), ('ford', 21), ('tire', 19), ('cite', 15), ('title', 15), ('bridgestone', 14), ('url', 14), ('www', 13), ('name', 13), ('com', 13), ('rubber', 12), ('accessdate', 11), ('web', 11), ('cnn', 11), ('publisher', 9), ('2007', 9), ('said', 8)]\n",
      "\n",
      "filtered and then stemmed:\n",
      " [('fireston', 76), ('tire', 43), ('ref', 32), ('compani', 24), ('ford', 21), ('titl', 15), ('cite', 15), ('url', 14), ('bridgeston', 14), ('www', 13), ('rubber', 13), ('com', 13), ('name', 13), ('accessd', 11), ('web', 11), ('cnn', 11), ('publish', 9), ('2007', 9), ('recal', 9), ('corpor', 9)]\n"
     ]
    }
   ],
   "source": [
    "# testing the functions\n",
    "tokens = get_tokens( data.text[1000])\n",
    "filtered = remove_stop_words(tokens)\n",
    "filstemed = text_clean(data.text[1000])\n",
    "cc = Counter(tokens)\n",
    "print('unfiltered:\\n',cc.most_common(20) )\n",
    "cc2 = Counter(filtered)\n",
    "print('\\nfiltered:\\n',cc2.most_common(20) )\n",
    "cc3 = Counter(filstemed)\n",
    "print('\\nfiltered and then stemmed:\\n',cc3.most_common(20) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# takes a while \n",
    "data.loc[:,'clean_text'] = data.text.apply(text_clean) \n",
    "data.to_pickle(\"cleaned-10k-articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

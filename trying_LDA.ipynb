{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA \n",
    "I am using the LDA package and following their main example [here](https://pypi.python.org/pypi/lda) closely.\n",
    "- The number of topics is set at 20.\n",
<<<<<<< HEAD
    "- with the 0.05 cutoff for vocab (which we are using elsewhere, and 20 topics), the algorithm gives results in the order of a minute (didn't time it.)"
=======
    "- with the 0.05 cutoff for vocab (which we are using elsewhere, and 20 topics), the algorithm gives results in the order of 5 minutes (didn't time it.)"
>>>>>>> 25e53859051cd325874b40ad05deea9fff22eb90
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import *\n",
    "import lda #use pip install lda \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"processed_10k_articles.pkl\")\n",
    "titles = [word for word in data.title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from Tristan's code:\n",
    "#putting the code first \n",
    "#first generate the bag of words.  This has no TF-IDF weighting yet.\n",
    "#Only include words that occur in at least 5% of documents.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",min_df=0.05)\n",
    "clean_text = data[\"process\"]\n",
    "unweighted_words = vectorizer.fit_transform(clean_text)\n",
    "terms_matrix = unweighted_words.toarray()\n",
    "vocabulary  = vectorizer.vocabulary_ # the words selected \n",
    "vocab = [w for w in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x10f9e72b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(terms_matrix)  # model.fit_transform(X) is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 209)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word = model.topic_word_ \n",
    "topic_word.shape # all elements are >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "doc_topic.shape # all elements are >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: top words: name ii form given song \n",
      " (article with highest weight: List of infantry guns)\n",
      "Topic 1: top words: april home centuri team high \n",
      " (article with highest weight: Ed Wood (movie))\n",
      "Topic 2: top words: import career august unit part \n",
      " (article with highest weight: List of cities in Senegal)\n",
      "Topic 3: top words: given famou caus novemb current \n",
      " (article with highest weight: Trams in London)\n",
      "Topic 4: top words: west footbal person japanes februari \n",
      " (article with highest weight: 2009 US Open â€“ Women's Singles)\n",
      "Topic 5: top words: move major star good presid \n",
      " (article with highest weight: Contract)\n",
      "Topic 6: top words: com end member releas develop \n",
      " (article with highest weight: Hurricane Keith)\n",
      "Topic 7: top words: term st career unit august \n",
      " (article with highest weight: List of comuni of the Province of Pesaro and Urbino)\n",
      "Topic 8: top words: much histori page way london \n",
      " (article with highest weight: C-type asteroid)\n",
      "Topic 9: top words: made near state statist sinc \n",
      " (article with highest weight: Communes of the Charente-Maritime department)\n",
      "Topic 10: top words: june british new great live \n",
      " (article with highest weight: April 4)\n",
      "Topic 11: top words: juli februari live die japanes \n",
      " (article with highest weight: Roberto Baggio)\n",
      "Topic 12: top words: death book john member right \n",
      " (article with highest weight: Yvain)\n",
      "Topic 13: top words: war http sever old world \n",
      " (article with highest weight: List of rivers of Georgia (U.S. state))\n",
      "Topic 14: top words: govern town system nation place \n",
      " (article with highest weight: A-Sides)\n",
      "Topic 15: top words: major la current list histori \n",
      " (article with highest weight: Finite set)\n",
      "Topic 16: top words: commun well game becam call \n",
      " (article with highest weight: First inauguration of Thomas Jefferson)\n",
      "Topic 17: top words: said top age french began \n",
      " (article with highest weight: Bernard Bailyn)\n",
      "Topic 18: top words: german word th style anoth \n",
      " (article with highest weight: List of places in New York: C)\n",
      "Topic 19: top words: type said would back studi \n",
      " (article with highest weight: Chancellor of the Exchequer)\n"
     ]
    }
   ],
   "source": [
    "# looking at the topics that are produced\n",
    "n_top_words = 5\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    tmp_topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: top words: {} \\n (article with highest weight: {})'.format(\\\n",
    "    i, ' '.join(tmp_topic_words), titles[doc_topic[:,i].argmax()]  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art (top topic: [14 12  5])\n",
      "Abbreviation (top topic: [12 16  5])\n",
      "Astronomy (top topic: [ 1  5 15])\n",
      "Browser (top topic: [ 8 19 15])\n",
      "Bubonic plague (top topic: [ 5 12  6])\n",
      "Cooking (top topic: [19  8  5])\n",
      "Calculus (top topic: [12  5 15])\n",
      "Coin (top topic: [16  8  5])\n",
      "Earth science (top topic: [19 12 15])\n",
      "Everything2 (top topic: [1 3 5])\n"
     ]
    }
   ],
   "source": [
    "# looking at articles\n",
    "for i in range(10):\n",
    "#     print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))\n",
    "     print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argsort()[-3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 40 most informative words:\n",
      "    import, highest-weight topic:  2, lowest-weight topic:15\n",
      "      term, highest-weight topic:  7, lowest-weight topic:15\n",
      "        st, highest-weight topic:  7, lowest-weight topic:15\n",
      "      juli, highest-weight topic: 11, lowest-weight topic:15\n",
      "      made, highest-weight topic:  9, lowest-weight topic:15\n",
      "    career, highest-weight topic:  2, lowest-weight topic:15\n",
      "    german, highest-weight topic: 18, lowest-weight topic:15\n",
      "      word, highest-weight topic: 18, lowest-weight topic:15\n",
      "    commun, highest-weight topic: 16, lowest-weight topic:5\n",
      "      june, highest-weight topic: 10, lowest-weight topic:15\n",
      "      name, highest-weight topic:  0, lowest-weight topic:15\n",
      "      west, highest-weight topic:  4, lowest-weight topic:15\n",
      "   footbal, highest-weight topic:  4, lowest-weight topic:15\n",
      "       war, highest-weight topic: 13, lowest-weight topic:15\n",
      "      well, highest-weight topic: 16, lowest-weight topic:15\n",
      "      said, highest-weight topic: 17, lowest-weight topic:15\n",
      "     april, highest-weight topic:  1, lowest-weight topic:15\n",
      "    govern, highest-weight topic: 14, lowest-weight topic:15\n",
      "     major, highest-weight topic: 15, lowest-weight topic:10\n",
      "       top, highest-weight topic: 17, lowest-weight topic:5\n",
      "      near, highest-weight topic:  9, lowest-weight topic:15\n",
      "       age, highest-weight topic: 17, lowest-weight topic:5\n",
      "  februari, highest-weight topic: 11, lowest-weight topic:15\n",
      "    person, highest-weight topic:  4, lowest-weight topic:15\n",
      "        ii, highest-weight topic:  0, lowest-weight topic:15\n",
      "     given, highest-weight topic:  3, lowest-weight topic:15\n",
      "        th, highest-weight topic: 18, lowest-weight topic:15\n",
      "      move, highest-weight topic:  5, lowest-weight topic:15\n",
      "   japanes, highest-weight topic:  4, lowest-weight topic:15\n",
      "      http, highest-weight topic: 13, lowest-weight topic:15\n",
      "      home, highest-weight topic:  1, lowest-weight topic:15\n",
      "     state, highest-weight topic:  9, lowest-weight topic:15\n",
      "   statist, highest-weight topic:  9, lowest-weight topic:15\n",
      "     sever, highest-weight topic: 13, lowest-weight topic:15\n",
      "      live, highest-weight topic: 11, lowest-weight topic:15\n",
      "      sinc, highest-weight topic:  9, lowest-weight topic:6\n",
      "      town, highest-weight topic: 14, lowest-weight topic:15\n",
      "    system, highest-weight topic: 14, lowest-weight topic:5\n",
      "    french, highest-weight topic: 17, lowest-weight topic:15\n",
      "       old, highest-weight topic: 13, lowest-weight topic:15\n"
     ]
    }
   ],
   "source": [
    "# top 40 most informative words in the vocabulary\n",
    "voc_var = [topic_word[:,i].var() for i in range(topic_word[:,:].shape[1]) ]\n",
    "tophowmany = 40\n",
    "print('top %i most informative words:'%tophowmany)\n",
    "top_informative_words = np.asarray(voc_var).argsort()[::-1][:tophowmany]\n",
    "for i in top_informative_words:\n",
    "    print(\"%10s, highest-weight topic:%3d, lowest-weight topic:%d\"%(vocab[i], topic_word[:,i].argmax(),  topic_word[:,i].argmin() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('topic_words_matrix',topic_word)\n",
    "np.save('topic_words_vocab',vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

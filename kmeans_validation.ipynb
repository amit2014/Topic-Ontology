{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of kmeans clustering\n",
    "Author: Tristan Miller\n",
    "\n",
    "I'm loosely following the validation method from Ryan P Adams' lecture notes, provided by Max.  The idea is to create test data with similar statistics to the real data, but with none of the clustering behavior.  Then kmeans is performed on the test data, and a clustering statistic is computed for both the test data and real data.  The best choice of k is that which maximizes the difference in the clustering statistic.\n",
    "\n",
    "From previous discussion, I thought the best way to generate test data would be to take real data, and shuffle the dimensions of each term vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import *\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"processed_10k_articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first generate the bag of words.  This has no TF-IDF weighting yet.\n",
    "#Only include words that occur in at least 5% of documents.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",min_df=0.05)\n",
    "clean_text = data[\"process\"]\n",
    "unweighted_words = vectorizer.fit_transform(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 209)\n",
      "['age', 'also', 'america', 'american', 'anoth', 'april', 'area', 'around', 'august', 'award', 'back', 'base', 'becam', 'becom', 'began', 'best', 'book', 'born', 'british', 'call', 'career', 'caus', 'center', 'centuri', 'chang', 'children', 'citi', 'class', 'club', 'com', 'come', 'common', 'commun', 'could', 'counti', 'countri', 'creat', 'current', 'day', 'de', 'death', 'decemb', 'develop', 'die', 'differ', 'earli', 'east', 'end', 'england', 'english', 'even', 'event', 'exampl', 'famili', 'famou', 'februari', 'first', 'follow', 'footbal', 'form', 'former', 'found', 'four', 'franc', 'french', 'game', 'gener', 'german', 'get', 'given', 'go', 'good', 'govern', 'great', 'group', 'help', 'high', 'histori', 'home', 'hous', 'howev', 'http', 'ii', 'import', 'includ', 'intern', 'januari', 'japanes', 'john', 'juli', 'june', 'kill', 'king', 'known', 'la', 'larg', 'last', 'later', 'leagu', 'left', 'life', 'like', 'list', 'live', 'london', 'long', 'made', 'main', 'major', 'make', 'man', 'mani', 'march', 'may', 'mean', 'member', 'move', 'movi', 'much', 'music', 'name', 'nation', 'near', 'new', 'north', 'novemb', 'number', 'octob', 'offici', 'often', 'old', 'one', 'open', 'origin', 'page', 'part', 'peopl', 'person', 'place', 'play', 'player', 'popul', 'popular', 'power', 'presid', 'produc', 'record', 'refer', 'region', 'relat', 'releas', 'right', 'river', 'rowspan', 'said', 'school', 'second', 'see', 'septemb', 'seri', 'set', 'sever', 'show', 'sinc', 'singl', 'small', 'sometim', 'song', 'south', 'st', 'star', 'start', 'state', 'statist', 'still', 'studi', 'style', 'system', 'take', 'team', 'televis', 'term', 'th', 'thing', 'three', 'time', 'top', 'total', 'town', 'two', 'type', 'unit', 'univers', 'use', 'usual', 'war', 'water', 'way', 'websit', 'well', 'west', 'wikit', 'word', 'work', 'world', 'would', 'www', 'year', 'york']\n"
     ]
    }
   ],
   "source": [
    "print(unweighted_words.shape)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason there are only 209 features now?  Digits are no longer included, but there are also other words missing compared to the previous list.\n",
    "\n",
    "I'm also worried by the appearance of www and wikit.  It seems that the regex isn't successfully filtering out everything yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF weighting can be applied after the fact with TfidfTransformer\n",
    "Tfidf = TfidfTransformer()\n",
    "#IDF weights only need to be calculated once, and can be reused for test data.\n",
    "Tfidf.fit(unweighted_words)\n",
    "#Now we apply the weights\n",
    "real_data = Tfidf.transform(unweighted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run kmeans on real data\n",
    "kmeans=KMeans(n_clusters=15)\n",
    "kmeans_pred=kmeans.fit_predict(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       term  frequency\n",
      "32   commun   0.486034\n",
      "63    franc   0.422043\n",
      "148  region   0.275081\n",
      "61    found   0.220306\n",
      "22   center   0.095247\n",
      "39       de   0.086594\n",
      "147   refer   0.058293\n",
      "124   north   0.043674\n",
      "168   south   0.041866\n",
      "64   french   0.037705\n",
      "['Henry II of France', 'Francis II of France', 'Region of Murcia', 'Medieval commune', 'Limoges', 'Champagne-Ardenne', 'Libourne', 'Luxembourg franc', 'French Community', 'Prime Minister of France', \"Albon-d'Ardèche\", 'Cellier-du-Luc', 'Charnas', 'Viviers, Ardèche', 'Rosières, Ardèche', 'Lake Annecy', 'Haute-Savoie', 'Province of Palermo', 'Province of Biella', 'Province of Genoa', 'Province of Bologna']\n"
     ]
    }
   ],
   "source": [
    "#print out some information about a cluster\n",
    "def cluster_info(cluster_index,kmeans,kmeans_pred):\n",
    "    #print out the top terms at the cluster center\n",
    "    cluster_mean = pd.DataFrame(index=range(real_data.shape[1]),columns=['term','frequency'])\n",
    "    cluster_mean['term']=vectorizer.get_feature_names()\n",
    "    cluster_mean['frequency']=kmeans.cluster_centers_[cluster_index]\n",
    "    cluster_mean.sort_values('frequency',ascending=False,inplace=True)\n",
    "    print(cluster_mean[0:10])\n",
    "\n",
    "    #print out some titles\n",
    "    print([data['title'][i] for i in range(len(data)) if kmeans_pred[i] == cluster_index][0:21])\n",
    "\n",
    "cluster_info(9,kmeans,kmeans_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import sqeuclidean\n",
    "\n",
    "#Now let's get the clustering statistic\n",
    "def cluster_statistic(bag_of_words,kmeans,kmeans_pred):\n",
    "    running_sum = [0]*len(kmeans.cluster_centers_)\n",
    "    \n",
    "    for i in range(len(kmeans_pred)):\n",
    "        #for each data point, add the square distance to the cluster center to the running sum\n",
    "        running_sum[kmeans_pred[i]] += sqeuclidean(kmeans.cluster_centers_[kmeans_pred[i]],bag_of_words[i,:].toarray())\n",
    "    #normalize each cluster to the size of the cluster\n",
    "    for k in range(len(running_sum)):\n",
    "        cluster_size = len([i for i in range(len(kmeans_pred)) if kmeans_pred[i] == k])\n",
    "        running_sum[k] /= cluster_size\n",
    "    return sum(running_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stat = cluster_statistic(real_data,kmeans,kmeans_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0165384481\n"
     ]
    }
   ],
   "source": [
    "print(stat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
